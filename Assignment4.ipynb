{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "# Leukemia Detection Analysis\n",
    "\n",
    "## Project Hypothesis\n",
    "**H₀ (Null Hypothesis):** The transfer learning-based model performs no better than the CNN based ML Model providing 82% F1 score in diagnosing Leukemia and differentiating it from other diseases.\n",
    "\n",
    "**H₁ (Alternative Hypothesis):** The transfer learning-based model provides better F1 score than the CNN based ML model in differentiating Acute Myeloid Leukemia from other common diseases.\n"
   ],
   "id": "df50788b644f97b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T17:47:37.375517Z",
     "start_time": "2025-03-11T17:47:33.959154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CODE CELL\n",
    "# Install required packages\n",
    "!pip install -q numpy pandas matplotlib seaborn scikit-learn scikit-image opencv-python tensorflow kagglehub\n",
    "\n",
    "# CODE CELL\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from google.colab import drive\n",
    "import random\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "id": "4ea58466194977cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "## Data Preparation\n",
    "\n",
    "### Mount Google Drive and Create Directory Structure\n"
   ],
   "id": "1092fa721c9b5c00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "# Mount Google Drive (required for Colab)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create a directory structure for the project\n",
    "!mkdir -p /content/leukemia_detection/data/L1\n",
    "!mkdir -p /content/leukemia_detection/data/L2\n",
    "!mkdir -p /content/leukemia_detection/data/L3"
   ],
   "id": "5e143916542feffa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "### Download and Prepare the Kaggle Dataset"
   ],
   "id": "a46154dacba1cbb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "# Install kagglehub for downloading dataset\n",
    "!pip install -q kagglehub\n",
    "\n",
    "# Download the dataset using kagglehub\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"sizlingdhairya1/all-idb-images\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Extract to a consistent location\n",
    "!mkdir -p /content/all-idb-images\n",
    "!cp -r $path/* /content/all-idb-images/\n",
    "\n",
    "print(\"Dataset downloaded and extracted.\")\n",
    "\n",
    "# CODE CELL\n",
    "# Create the folder structure and distribute images for the project\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Get all image paths\n",
    "all_images = glob.glob('/content/all-idb-images/*/im/*.jpg')\n",
    "print(f\"Total images found: {len(all_images)}\")\n",
    "\n",
    "# Ensure we have at least 150 images\n",
    "if len(all_images) >= 150:\n",
    "    # Randomly select 150 images and distribute them evenly\n",
    "    selected_images = random.sample(all_images, 150)\n",
    "\n",
    "    # 50 images for each folder\n",
    "    for i, img_path in enumerate(selected_images):\n",
    "        if i < 50:\n",
    "            dest_folder = '/content/leukemia_detection/data/L1'\n",
    "        elif i < 100:\n",
    "            dest_folder = '/content/leukemia_detection/data/L2'\n",
    "        else:\n",
    "            dest_folder = '/content/leukemia_detection/data/L3'\n",
    "\n",
    "        # Copy the image to the destination folder\n",
    "        shutil.copy(img_path, dest_folder)\n",
    "\n",
    "    print(\"Images distributed into L1, L2, and L3 folders.\")\n",
    "else:\n",
    "    print(f\"Not enough images found. Only {len(all_images)} images available.\")\n"
   ],
   "id": "9c9683245d92474f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# MARKDOWN CELL\n",
    "## Part 1: Image Dataset Exploratory Data Analysis\n",
    "\n",
    "### Loading and Visualizing Images"
   ],
   "id": "4ba7e97c3783551c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "def load_images_from_folder(folder):\n",
    "    \"\"\"Load all images from a folder into a list.\"\"\"\n",
    "    images = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            # Convert to RGB (OpenCV loads as BGR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "            filenames.append(filename)\n",
    "    return images, filenames\n",
    "\n",
    "# Load images from each folder\n",
    "print(\"Loading images...\")\n",
    "l1_images, l1_filenames = load_images_from_folder('/content/leukemia_detection/data/L1')\n",
    "l2_images, l2_filenames = load_images_from_folder('/content/leukemia_detection/data/L2')\n",
    "l3_images, l3_filenames = load_images_from_folder('/content/leukemia_detection/data/L3')\n",
    "\n",
    "print(f\"L1 images: {len(l1_images)}\")\n",
    "print(f\"L2 images: {len(l2_images)}\")\n",
    "print(f\"L3 images: {len(l3_images)}\")\n",
    "\n",
    "# CODE CELL\n",
    "# Visualize sample images from each folder\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (folder_images, folder_name) in enumerate(zip([l1_images, l2_images, l3_images], ['L1', 'L2', 'L3'])):\n",
    "    if len(folder_images) > 0:\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.imshow(folder_images[0])\n",
    "        plt.title(f\"Sample from {folder_name}\")\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_images.png')\n",
    "plt.show()"
   ],
   "id": "be3e23f7aae5f63e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "### Image Properties Analysis"
   ],
   "id": "e47413aef3946ee7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "# Analyze image properties (size, channels, etc.)\n",
    "def analyze_image_properties(image_list, folder_name):\n",
    "    \"\"\"Analyze and display properties of images in a folder.\"\"\"\n",
    "    sizes = []\n",
    "    channels = []\n",
    "    avg_intensity = []\n",
    "    std_intensity = []\n",
    "\n",
    "    for img in image_list:\n",
    "        h, w, c = img.shape\n",
    "        sizes.append((h, w))\n",
    "        channels.append(c)\n",
    "        avg_intensity.append(np.mean(img))\n",
    "        std_intensity.append(np.std(img))\n",
    "\n",
    "    print(f\"\\n--- {folder_name} Image Properties ---\")\n",
    "    print(f\"Number of images: {len(image_list)}\")\n",
    "    print(f\"Image dimensions (height, width): {sizes[0]} (first image)\")\n",
    "    print(f\"Unique image sizes: {len(set(sizes))}\")\n",
    "    print(f\"Number of channels: {channels[0]}\")\n",
    "    print(f\"Average pixel intensity: {np.mean(avg_intensity):.2f}\")\n",
    "    print(f\"Average standard deviation: {np.mean(std_intensity):.2f}\")\n",
    "\n",
    "    return {\n",
    "        'folder': folder_name,\n",
    "        'num_images': len(image_list),\n",
    "        'avg_intensity': np.mean(avg_intensity),\n",
    "        'std_intensity': np.mean(std_intensity)\n",
    "    }\n",
    "\n",
    "# Analyze images from each folder\n",
    "l1_stats = analyze_image_properties(l1_images, 'L1')\n",
    "l2_stats = analyze_image_properties(l2_images, 'L2')\n",
    "l3_stats = analyze_image_properties(l3_images, 'L3')\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "stats_df = pd.DataFrame([l1_stats, l2_stats, l3_stats])\n",
    "\n",
    "# CODE CELL\n",
    "# Visualize image statistics\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Average intensity\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='folder', y='avg_intensity', data=stats_df)\n",
    "plt.title('Average Pixel Intensity')\n",
    "plt.ylim(0, 255)\n",
    "\n",
    "# Standard deviation\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='folder', y='std_intensity', data=stats_df)\n",
    "plt.title('Average Standard Deviation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('image_statistics.png')\n",
    "plt.show()\n"
   ],
   "id": "f90f5cdd17d03ca3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# MARKDOWN CELL\n",
    "### Color Distribution Analysis\n"
   ],
   "id": "e1ccdf9f58894756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "# Analyze color distribution in images\n",
    "def plot_color_histograms(images, folder_name):\n",
    "    \"\"\"Plot color histograms for a sample of images from a folder.\"\"\"\n",
    "    # Sample up to 5 images\n",
    "    sample_images = random.sample(images, min(5, len(images)))\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(f'Color Histograms for {folder_name}', fontsize=16)\n",
    "\n",
    "    for i, img in enumerate(sample_images):\n",
    "        colors = ('r', 'g', 'b')\n",
    "        for j, color in enumerate(colors):\n",
    "            plt.subplot(len(sample_images), 3, i*3 + j + 1)\n",
    "            histogram = cv2.calcHist([img], [j], None, [256], [0, 256])\n",
    "            plt.plot(histogram, color=color)\n",
    "            plt.xlim([0, 256])\n",
    "            if i == 0:\n",
    "                plt.title(f'{color.upper()} Channel')\n",
    "            if j == 0:\n",
    "                plt.ylabel(f'Image {i+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(f'{folder_name}_color_histograms.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot color histograms for each folder\n",
    "for images, folder_name in zip([l1_images, l2_images, l3_images], ['L1', 'L2', 'L3']):\n",
    "    if len(images) > 0:\n",
    "        plot_color_histograms(images, folder_name)\n"
   ],
   "id": "118834a023ba7bd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "### Structural Similarity Index (SSIM) Analysis\n"
   ],
   "id": "faa83cee86286191"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "def calculate_avg_ssim(images_list1, images_list2, n_samples=5):\n",
    "    \"\"\"Calculate average SSIM between two sets of images.\"\"\"\n",
    "    if len(images_list1) == 0 or len(images_list2) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Resize images to a common size for comparison\n",
    "    size = (256, 256)\n",
    "\n",
    "    # Sample images from both lists\n",
    "    samples1 = random.sample(images_list1, min(n_samples, len(images_list1)))\n",
    "    samples2 = random.sample(images_list2, min(n_samples, len(images_list2)))\n",
    "\n",
    "    ssim_values = []\n",
    "\n",
    "    for img1 in samples1:\n",
    "        img1_resized = cv2.resize(img1, size)\n",
    "        img1_gray = cv2.cvtColor(img1_resized, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        for img2 in samples2:\n",
    "            img2_resized = cv2.resize(img2, size)\n",
    "            img2_gray = cv2.cvtColor(img2_resized, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "            # Calculate SSIM\n",
    "            ssim_value = ssim(img1_gray, img2_gray, data_range=img2_gray.max() - img2_gray.min())\n",
    "            ssim_values.append(ssim_value)\n",
    "\n",
    "    return np.mean(ssim_values)\n",
    "\n",
    "# Calculate SSIM between image groups\n",
    "ssim_l1_l2 = calculate_avg_ssim(l1_images, l2_images)\n",
    "ssim_l1_l3 = calculate_avg_ssim(l1_images, l3_images)\n",
    "ssim_l2_l3 = calculate_avg_ssim(l2_images, l3_images)\n",
    "\n",
    "# Calculate SSIM within groups (internal similarity)\n",
    "ssim_l1_l1 = calculate_avg_ssim(l1_images, l1_images)\n",
    "ssim_l2_l2 = calculate_avg_ssim(l2_images, l2_images)\n",
    "ssim_l3_l3 = calculate_avg_ssim(l3_images, l3_images)\n",
    "\n",
    "print(\"\\n--- Structural Similarity Index (SSIM) Analysis ---\")\n",
    "print(f\"SSIM between L1 and L2: {ssim_l1_l2:.4f}\")\n",
    "print(f\"SSIM between L1 and L3: {ssim_l1_l3:.4f}\")\n",
    "print(f\"SSIM between L2 and L3: {ssim_l2_l3:.4f}\")\n",
    "print(\"\\nInternal similarity:\")\n",
    "print(f\"SSIM within L1: {ssim_l1_l1:.4f}\")\n",
    "print(f\"SSIM within L2: {ssim_l2_l2:.4f}\")\n",
    "print(f\"SSIM within L3: {ssim_l3_l3:.4f}\")\n",
    "\n",
    "# CODE CELL\n",
    "# Visualize SSIM results\n",
    "ssim_data = [\n",
    "    ['L1-L2', ssim_l1_l2],\n",
    "    ['L1-L3', ssim_l1_l3],\n",
    "    ['L2-L3', ssim_l2_l3],\n",
    "    ['L1-L1', ssim_l1_l1],\n",
    "    ['L2-L2', ssim_l2_l2],\n",
    "    ['L3-L3', ssim_l3_l3]\n",
    "]\n",
    "\n",
    "ssim_df = pd.DataFrame(ssim_data, columns=['Comparison', 'SSIM'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Comparison', y='SSIM', data=ssim_df)\n",
    "plt.title('Structural Similarity Index (SSIM) Between Image Groups')\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig('ssim_comparison.png')\n",
    "plt.show()\n"
   ],
   "id": "8192abb97d12794c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# MARKDOWN CELL\n",
    "## Part 2: Synthetic Numerical Dataset Creation and Analysis\n",
    "\n",
    "### Create Synthetic Dataset"
   ],
   "id": "6add378aa71752e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "# Create a synthetic numerical dataset for leukemia detection\n",
    "def create_synthetic_dataset(n_samples=7257):\n",
    "    \"\"\"Create a synthetic dataset for leukemia detection with realistic features.\"\"\"\n",
    "\n",
    "    # Define features and their distributions\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Generate data\n",
    "    data = {\n",
    "        'patient_id': range(1, n_samples + 1),\n",
    "        'age': np.random.normal(45, 15, n_samples).clip(2, 90).astype(int),\n",
    "        'wbc_count': np.random.normal(20000, 15000, n_samples).clip(1000, 100000),\n",
    "        'rbc_count': np.random.normal(3.5, 1.0, n_samples).clip(1.0, 7.0),\n",
    "        'hemoglobin': np.random.normal(10.5, 2.5, n_samples).clip(4.0, 17.0),\n",
    "        'platelets': np.random.normal(120000, 80000, n_samples).clip(10000, 450000),\n",
    "        'blast_cells_percentage': np.random.normal(15, 20, n_samples).clip(0, 100),\n",
    "        'lymphocytes_percentage': np.random.normal(25, 15, n_samples).clip(0, 90),\n",
    "        'neutrophils_percentage': np.random.normal(35, 20, n_samples).clip(0, 85)\n",
    "    }\n",
    "\n",
    "    # Generate a target variable based on features\n",
    "    # High blast cell percentage and low RBC/hemoglobin are indicators of leukemia\n",
    "    leukemia_score = (\n",
    "        data['blast_cells_percentage'] / 100 * 3 -\n",
    "        data['rbc_count'] / 7.0 * 1.5 -\n",
    "        data['hemoglobin'] / 17.0 * 1.5 +\n",
    "        data['wbc_count'] / 100000 * 2\n",
    "    )\n",
    "\n",
    "    # Normalize score to 0-1 range\n",
    "    leukemia_score = (leukemia_score - leukemia_score.min()) / (leukemia_score.max() - leukemia_score.min())\n",
    "\n",
    "    # Generate target based on score\n",
    "    # 0: Normal, 1: Suspected, 2: AML (Acute Myeloid Leukemia), 3: Other blood disorders\n",
    "    thresholds = [0.3, 0.6, 0.85]\n",
    "    data['diagnosis'] = np.digitize(leukemia_score, thresholds)\n",
    "\n",
    "    # Convert diagnosis to labels\n",
    "    diagnosis_map = {0: 'Normal', 1: 'Suspected', 2: 'AML', 3: 'Other'}\n",
    "    data['diagnosis_label'] = [diagnosis_map[d] for d in data['diagnosis']]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Add some random noise and missing values to make dataset more realistic\n",
    "    for col in ['wbc_count', 'rbc_count', 'hemoglobin', 'platelets']:\n",
    "        mask = np.random.random(n_samples) < 0.01  # 1% missing values\n",
    "        df.loc[mask, col] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "numerical_df = create_synthetic_dataset(7257)\n",
    "\n",
    "# Save to CSV\n",
    "numerical_df.to_csv('leukemia_numerical_data.csv', index=False)\n",
    "print(\"Synthetic numerical dataset created and saved.\")\n",
    "print(f\"Dataset shape: {numerical_df.shape}\")\n",
    "numerical_df.head()\n"
   ],
   "id": "e43b2880557a6297"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "### Exploratory Data Analysis of Numerical Dataset\n"
   ],
   "id": "ff9c535baf3e3e83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "# Basic statistics\n",
    "print(\"\\n--- Numerical Dataset Statistics ---\")\n",
    "print(numerical_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(numerical_df.isnull().sum())\n",
    "\n",
    "# Distribution of diagnosis\n",
    "print(\"\\n--- Diagnosis Distribution ---\")\n",
    "diagnosis_counts = numerical_df['diagnosis_label'].value_counts()\n",
    "print(diagnosis_counts)\n",
    "\n",
    "# CODE CELL\n",
    "# Visualize diagnosis distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='diagnosis_label', data=numerical_df)\n",
    "plt.title('Distribution of Diagnosis')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('diagnosis_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# CODE CELL\n",
    "# Visualize feature distributions by diagnosis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# WBC count\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='diagnosis_label', y='wbc_count', data=numerical_df)\n",
    "plt.title('WBC Count by Diagnosis')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Blast cells percentage\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(x='diagnosis_label', y='blast_cells_percentage', data=numerical_df)\n",
    "plt.title('Blast Cells Percentage by Diagnosis')\n",
    "\n",
    "# Hemoglobin\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(x='diagnosis_label', y='hemoglobin', data=numerical_df)\n",
    "plt.title('Hemoglobin by Diagnosis')\n",
    "\n",
    "# Platelets\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(x='diagnosis_label', y='platelets', data=numerical_df)\n",
    "plt.title('Platelets by Diagnosis')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('features_by_diagnosis.png')\n",
    "plt.show()\n",
    "\n",
    "# CODE CELL\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = numerical_df.select_dtypes(include=[np.number]).drop(['patient_id', 'diagnosis'], axis=1).corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# CODE CELL\n",
    "# Feature relationships\n",
    "plt.figure(figsize=(15, 15))\n",
    "features = ['wbc_count', 'rbc_count', 'hemoglobin', 'platelets', 'blast_cells_percentage']\n",
    "sns.pairplot(numerical_df[features + ['diagnosis_label']], hue='diagnosis_label', corner=True)\n",
    "plt.savefig('feature_relationships.png')\n",
    "plt.show()\n"
   ],
   "id": "f88a31334a105e93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "## Part 3: Model Training and Hypothesis Testing\n",
    "\n",
    "### CNN-based Model (Baseline)"
   ],
   "id": "a711780103d4aab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "# Prepare image data for modeling\n",
    "def prepare_image_data(base_path, img_size=(150, 150), batch_size=32):\n",
    "    \"\"\"Prepare image data for model training.\"\"\"\n",
    "\n",
    "    # Data augmentation for training set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Only rescaling for validation set\n",
    "    val_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Load training data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        base_path,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Load validation data\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        base_path,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Prepare the data\n",
    "img_size = (150, 150)\n",
    "batch_size = 32\n",
    "train_generator, validation_generator = prepare_image_data('/content/leukemia_detection/data', img_size, batch_size)\n",
    "\n",
    "# CODE CELL\n",
    "# Build a basic CNN model\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"Build a basic CNN model.\"\"\"\n",
    "    model = Sequential([\n",
    "        # First convolutional block\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # Second convolutional block\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # Third convolutional block\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # Flatten and dense layers\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the CNN model\n",
    "input_shape = img_size + (3,)  # (150, 150, 3)\n",
    "num_classes = len(train_generator.class_indices)\n",
    "cnn_model = build_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Print model summary\n",
    "cnn_model.summary()\n",
    "\n",
    "# CODE CELL\n",
    "# Train the CNN model\n",
    "cnn_history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the CNN model\n",
    "cnn_evaluation = cnn_model.evaluate(validation_generator)\n",
    "print(f\"\\nCNN Model Evaluation:\")\n",
    "print(f\"Loss: {cnn_evaluation[0]:.4f}\")\n",
    "print(f\"Accuracy: {cnn_evaluation[1]:.4f}\")\n",
    "print(f\"Precision: {cnn_evaluation[2]:.4f}\")\n",
    "print(f\"Recall: {cnn_evaluation[3]:.4f}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "cnn_f1 = 2 * (cnn_evaluation[2] * cnn_evaluation[3]) / (cnn_evaluation[2] + cnn_evaluation[3])\n",
    "print(f\"F1 Score: {cnn_f1:.4f}\")\n",
    "\n",
    "# CODE CELL\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cnn_history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(cnn_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('CNN Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cnn_history.history['loss'], label='Train Loss')\n",
    "plt.plot(cnn_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('CNN Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_training_history.png')\n",
    "plt.show()\n"
   ],
   "id": "7687e3b40be8e211"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "### Transfer Learning Model\n"
   ],
   "id": "6fef0366efa7202e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CODE CELL\n",
    "# Build a transfer learning model using VGG16\n",
    "def build_transfer_learning_model(input_shape, num_classes):\n",
    "    \"\"\"Build a transfer learning model using pre-trained VGG16.\"\"\"\n",
    "    # Load pre-trained VGG16 model\n",
    "    base_model = VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the transfer learning model\n",
    "tl_model = build_transfer_learning_model(input_shape, num_classes)\n",
    "\n",
    "# Print model summary\n",
    "tl_model.summary()\n",
    "\n",
    "# CODE CELL\n",
    "# Train the transfer learning model\n",
    "tl_history = tl_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the transfer learning model\n",
    "tl_evaluation = tl_model.evaluate(validation_generator)\n",
    "print(f\"\\nTransfer Learning Model Evaluation:\")\n",
    "print(f\"Loss: {tl_evaluation[0]:.4f}\")\n",
    "print(f\"Accuracy: {tl_evaluation[1]:.4f}\")\n",
    "print(f\"Precision: {tl_evaluation[2]:.4f}\")\n",
    "print(f\"Recall: {tl_evaluation[3]:.4f}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "tl_f1 = 2 * (tl_evaluation[2] * tl_evaluation[3]) / (tl_evaluation[2] + tl_evaluation[3])\n",
    "print(f\"F1 Score: {tl_f1:.4f}\")\n",
    "\n",
    "# CODE CELL\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(tl_history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(tl_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Transfer Learning Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(tl_history.history['loss'], label='Train Loss')\n",
    "plt.plot(tl_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Transfer Learning Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tl_training_history.png')\n",
    "plt.show()\n"
   ],
   "id": "22b1b4cc6b4eb16a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MARKDOWN CELL\n",
    "## Hypothesis Testing\n",
    "### Statistical Test of Model Comparison\n",
    "\n"
   ],
   "id": "9c803c35b526e7a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CODE CELL\n",
    "# Compare F1 scores of the two models\n",
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(f\"CNN Model F1 Score: {cnn_f1:.4f}\")\n",
    "print(f\"Transfer Learning Model F1 Score: {tl_f1:.4f}\")\n",
    "print(f\"Difference in F1 Score: {tl_f1 - cnn_f1:.4f}\")\n",
    "\n",
    "# Perform statistical test (bootstrap sampling)\n",
    "def bootstrap_f1_scores(model, data_generator, n_samples=1000):\n",
    "    \"\"\"Generate bootstrap samples of F1 scores.\"\"\"\n",
    "    f1_scores = []\n",
    "\n",
    "    # Reset generator to the beginning\n",
    "    data_generator.reset()\n",
    "\n",
    "    # Get predictions for all validation samples\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(data_generator)):\n",
    "        x_batch, y_batch = data_generator[i]\n",
    "        pred_batch = model.predict(x_batch)\n",
    "\n",
    "        y_true.extend(np.argmax(y_batch, axis=1))\n",
    "        y_pred.extend(np.argmax(pred_batch, axis=1))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Bootstrapping\n",
    "    n_samples_total = len(y_true)\n",
    "    for _ in range(n_samples):\n",
    "        # Sample with replacement\n",
    "        indices = np.random.choice(n_samples_total, n_samples_total, replace=True)\n",
    "        y_true_sample = y_true[indices]\n",
    "        y_pred_sample = y_pred[indices]\n",
    "\n",
    "        # Calculate F1 score for this sample\n",
    "        f1 = f1_score(y_true_sample, y_pred_sample, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return np.array(f1_scores)\n",
    "\n",
    "# Generate bootstrap samples\n",
    "cnn_f1_samples = bootstrap_f1_scores(cnn_model, validation_generator)\n",
    "tl_f1_samples = bootstrap_f1_scores(tl_model, validation_generator)\n",
    "\n",
    "# Calculate p-value\n",
    "diff_samples = tl_f1_samples - cnn_f1_samples\n",
    "p_value = np.mean(diff_samples <= 0)\n",
    "\n",
    "print(f\"Bootstrap p-value: {p_value:.4f}\")\n",
    "\n",
    "# Set significance level\n",
    "alpha = 0.05\n",
    "print(f\"Significance level: {alpha}\")\n",
    "\n",
    "# Test the hypothesis\n",
    "if p_value < alpha:\n",
    "    print(\"\\nResult: Reject the null hypothesis.\")\n",
    "    print(\"The transfer learning-based model performs significantly better than the CNN-based model.\")\n",
    "else:\n",
    "    print(\"\\nResult: Fail to reject the null hypothesis.\")\n",
    "    print(\"There is not enough evidence to conclude that the transfer learning-based model performs better.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
